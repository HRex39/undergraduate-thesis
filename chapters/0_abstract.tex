%%
% The BIThesis Template for Bachelor Graduation Thesis
%
% 北京理工大学毕业设计（论文）中英文摘要 —— 使用 XeLaTeX 编译
%
% Copyright 2020-2021 BITNP
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Feng Kaiyu.

% 中英文摘要章节
\zihao{-4}
\vspace*{-11mm}

\begin{center}
  \heiti\zihao{-2}\textbf{\thesisTitle}
\end{center}

\vspace*{2mm}

{\let\clearpage\relax \chapter*{\textmd{摘~~~~要}}}
\addcontentsline{toc}{chapter}{摘~~~~要}
\setcounter{page}{1}

\vspace*{1mm}

\setstretch{1.53}
\setlength{\parskip}{0em}

% 中文摘要正文从这里开始
自动驾驶系统保证了快捷、安全、高效的驾驶体验，实现自动驾驶避障需要自动驾驶决策和控制系统的紧密配合。强化学习通过不断探索环境，自主学习复杂的控制模型，深度学习与强化学习相结合形成的深度强化学习方法可实现端到端的决策与控制，逐渐成为自动驾驶领域的研究热点。本文以实现端到端的自动驾驶决策器和控制器作为研究目标，围绕两种不同复杂度的仿真环境，针对DQN及其改进算法搭建神经网络模型开展了相关技术研究，主要研究内容如下：

（1）针对强化学习的相关算法选择，第2章通过对强化学习算法原理的分析，引出了适用于无模型的 Q-learning 算法和 DQN 算法，针对于DQN算法的不足和缺陷，介绍了Double DQN算法和Dueling DQN算法的原理，并对DQN及其改进算法的适用场景进行了分析。

（2）针对自动驾驶避障算法，第3章完成了自动驾驶决策器与控制器的算法设计。通过对两种仿真环境（Highway-Env、Metadrive）状态值、动作值与奖励函数的对比与分析，设计了DQN及其改进算法的网络结构及超参数，对自动驾驶决策器与控制器的学习过程进行分析，使其完成自动驾驶决策和控制两方面的任务要求。

（3）针对实验验证，第4章进行了两种仿真环境中决策器与控制器的实验，验证了提出的自动驾驶避障算法的可行性和有效性，并对具体的实验内容进行了设计，对最终的实验结果进行了详细的分析与改进。

实验结果表明，本文设计实现的基于DQN及其改进算法的自动驾驶决策器和控制器均满足实验要求，达到了预期的控制效果，能够有效提高自动驾驶车辆在决策和控制中的鲁棒性。对于较为简单的网络结构，DQN特别是Double DQN算法由于其改进的动作选择和评估方法，能够获得更加稳定有效的行为策略。本文的成果为自动驾驶决策器和控制器的研究提供了借鉴和参考，也为复杂动力学模型问题的解决提供了新的思路和方法。

\vspace{4ex}\noindent\textbf{\heiti 关键词：自动驾驶避障；自动驾驶决策；控制器设计；DQN网络；深度强化学习；端到端驾驶}
\newpage

% 英文摘要章节
\vspace*{-2mm}

\begin{spacing}{0.95}
  \centering
  \heiti\zihao{3}\textbf{\thesisTitleEN}
\end{spacing}

\vspace*{17mm}

{\let\clearpage\relax \chapter*{
  \zihao{-3}\textmd{Abstract}\vskip -3bp}}
\addcontentsline{toc}{chapter}{Abstract}
\setcounter{page}{2}

\setstretch{1.53}
\setlength{\parskip}{0em}

% 英文摘要正文从这里开始
The Automatic Driving System ensures a fast, safe and efficient driving experience. The implementation of Automatic Driving Obstacle Avoidance requires the cooperation of the decision-making and control system of Automatic Driving. Reinforcement Learning learns complex control models autonomously by exploring the environment continuously. The Deep Reinforcement Learning method, formed by the combination of Deep Learning and Reinforcement Learning,  can realize end-to-end decision-making and control, and has gradually become a research hotspot in the field of Autonomous Driving. This work aims to implement the end-to-end Autonomous Driving decision maker and controller. Focusing on two simulation environments with different complexity, a neural network model is built for DQN and its improved algorithms. The performance is verified. The main research contents are as follows:

(1) For the selection of relevant algorithms for Reinforcement Learning, the second chapter analyses the principle of Reinforcement Learning algorithm, introduces the Q-learning algorithm and DQN algorithm suitable for model-free. In view of the shortcomings and defects of DQN algorithm, the principles of Double DQN algorithm and Dueling DQN algorithm are introduced, and the applicable scenarios of DQN and its improved algorithm are analyzed.

(2) For the Automatic Driving Obstacle Avoidance algorithm, the third chapter completes the algorithm design of the Automatic Driving decision maker and controller. Through the comparison and analysis of the state value, action value and reward function of the two simulation environments (Highway-Env, Metadrive), the network structure and hyperparameters of DQN and its improved algorithm are designed. 

(3) For the experimental verification, the fourth chapter conducts experiments on the decision maker and controller in two simulation environments, verifies the feasibility and effectiveness of the proposed Automatic Driving Obstacle Avoidance algorithm, the final experimental results are analyzed and improved in detail.

The experimental results show that the Autonomous Driving decision maker and controller based on DQN and its improved algorithms designed and implemented in this paper meet the experimental requirements, achieve the expected control effect, and can effectively improve the robustness of autonomous driving vehicles in decision-making and control. For simpler network structures, DQN, especially the Double DQN algorithm, can obtain more stable and effective behavior strategies due to its improved action selection and evaluation methods. The results of this paper provide reference for the research of Automatic Driving decision maker and controller, and also provide new ideas and methods for solving complex dynamic model problems.

\vspace{3ex}\noindent\textbf{Key Words: Automatic Driving Obstacle Avoidance; Automatic Driving Decision-Making; Controller Design; DQN Network; Reinforcement Learning; End-to-end Driving}
\newpage
